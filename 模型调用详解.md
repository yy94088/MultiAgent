# AgentPrune 模型调用系统详解

## 目录
1. [系统架构概览](#系统架构概览)
2. [LLM调用层次结构](#llm调用层次结构)
3. [模型配置与初始化](#模型配置与初始化)
4. [Prompt构建机制](#prompt构建机制)
5. [实际调用流程](#实际调用流程)
6. [完整调用链路追踪](#完整调用链路追踪)

---

## 系统架构概览

### 整体架构图

```
┌─────────────────────────────────────────────────────────────┐
│                    Agent执行层                                │
│  (MathSolver, FinalRefer, etc.)                             │
└────────────────┬────────────────────────────────────────────┘
                 │ 调用 llm.agen(message)
                 ↓
┌─────────────────────────────────────────────────────────────┐
│                   LLM抽象层                                   │
│  LLMRegistry.get(model_name) → LLM实例                       │
└────────────────┬────────────────────────────────────────────┘
                 │ 返回具体实现
                 ↓
┌─────────────────────────────────────────────────────────────┐
│                  GPTChat实现层                                │
│  GPTChat.agen() → achat()                                   │
└────────────────┬────────────────────────────────────────────┘
                 │ 异步HTTP调用
                 ↓
┌─────────────────────────────────────────────────────────────┐
│                 OpenAI API层                                 │
│  AsyncOpenAI.chat.completions.create()                      │
└────────────────┬────────────────────────────────────────────┘
                 │ 网络请求
                 ↓
┌─────────────────────────────────────────────────────────────┐
│              实际模型服务                                      │
│  (OpenAI API / 本地vLLM / 其他兼容服务)                        │
└─────────────────────────────────────────────────────────────┘
```

---

## LLM调用层次结构

### 1. LLM基类 (llm.py)

**定义抽象接口，所有LLM实现必须遵循此接口**

```python
from abc import ABC, abstractmethod
from typing import List, Union, Optional
from AgentPrune.llm.format import Message

class LLM(ABC):
    # 默认配置
    DEFAULT_MAX_TOKENS = 1000        # 最大生成token数
    DEFAULT_TEMPERATURE = 0.2        # 温度参数（较低=更确定性）
    DEFUALT_NUM_COMPLETIONS = 1      # 生成的回复数量

    @abstractmethod
    async def agen(
        self,
        messages: List[Message],      # 对话消息列表
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        num_comps: Optional[int] = None,
    ) -> Union[List[str], str]:
        """异步生成回复"""
        pass

    @abstractmethod
    def gen(
        self,
        messages: List[Message],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        num_comps: Optional[int] = None,
    ) -> Union[List[str], str]:
        """同步生成回复（当前未实现）"""
        pass
```

#### 设计要点

- **抽象基类**：使用ABC确保子类必须实现关键方法
- **可选参数**：支持覆盖默认配置
- **类型提示**：明确输入输出类型
- **双模式**：支持同步和异步调用（但项目主要使用异步）

---

### 2. LLM注册表 (llm_registry.py)

**提供统一的LLM实例获取接口**

```python
from typing import Optional
from class_registry import ClassRegistry
from AgentPrune.llm.llm import LLM

class LLMRegistry:
    # 使用class_registry管理所有LLM实现
    registry = ClassRegistry()

    @classmethod
    def register(cls, *args, **kwargs):
        """装饰器：注册LLM实现类"""
        return cls.registry.register(*args, **kwargs)
    
    @classmethod
    def keys(cls):
        """获取所有已注册的LLM名称"""
        return cls.registry.keys()

    @classmethod
    def get(cls, model_name: Optional[str] = None) -> LLM:
        """根据模型名称获取LLM实例"""
        # 默认使用gpt-4o
        if model_name is None or model_name == "":
            model_name = "gpt-4o"

        # Mock模型（用于测试）
        if model_name == 'mock':
            model = cls.registry.get(model_name)
        else:
            # 所有GPT系列模型都使用GPTChat类
            # 第二个参数是模型名称，传递给GPTChat构造函数
            model = cls.registry.get('GPTChat', model_name)

        return model
```

#### 工作原理

1. **注册机制**：通过装饰器 `@LLMRegistry.register('GPTChat')` 注册实现类
2. **工厂模式**：`get()` 方法根据名称返回对应实例
3. **统一接口**：所有GPT系列模型（gpt-4o, gpt-4-turbo, gpt-3.5等）都映射到 `GPTChat` 类
4. **可扩展**：可以轻松添加新的LLM实现（如Claude, Llama等）

---

### 3. GPTChat实现 (gpt_chat.py)

**具体的OpenAI API调用实现**

#### 环境配置

```python
from dotenv import load_dotenv
import os
from openai import OpenAI, AsyncOpenAI

# 默认配置（如果不使用.env文件）
OPENAI_API_KEYS = ['']
BASE_URL = ''

# 加载.env文件
load_dotenv()
MINE_BASE_URL = os.getenv('BASE_URL')  # 例如：http://localhost:8000/v1
MINE_API_KEY = os.getenv('API_KEY')    # 例如：EMPTY 或实际API密钥
```

#### 核心函数：achat

```python
from tenacity import retry, wait_random_exponential, stop_after_attempt

@retry(
    wait=wait_random_exponential(max=300),  # 指数退避，最多等待300秒
    stop=stop_after_attempt(3)              # 最多重试3次
)
async def achat(
    model: str,      # 模型名称，如 "gpt-4o", "Qwen3-8B"
    msg: List[Dict], # 消息列表，格式：[{'role':'user', 'content':'...'}]
):
    """异步调用OpenAI兼容的API"""
    
    # 创建异步客户端
    client = AsyncOpenAI(
        base_url=MINE_BASE_URL,  # API端点
        api_key=MINE_API_KEY,    # 认证密钥
    )
    
    # 发起聊天完成请求
    chat_completion = await client.chat.completions.create(
        messages=msg,
        model=model,
    )
    
    # 提取响应内容
    response = chat_completion.choices[0].message.content
    
    return response
```

**重试机制说明**：
- 使用 `tenacity` 库实现自动重试
- **指数退避**：第1次失败后等待短时间，第2次等待更长时间，以此类推
- **最大重试次数**：3次，如果3次都失败则抛出异常
- **适用场景**：网络波动、API限流、临时服务不可用

#### GPTChat类实现

```python
@LLMRegistry.register('GPTChat')
class GPTChat(LLM):

    def __init__(self, model_name: str):
        """
        初始化GPTChat实例
        
        Args:
            model_name: 模型名称，如 "gpt-4o", "Qwen3-8B"
        """
        self.model_name = model_name

    async def agen(
        self,
        messages: List[Message],               # 消息列表
        max_tokens: Optional[int] = None,      # 最大生成token数
        temperature: Optional[float] = None,   # 温度参数
        num_comps: Optional[int] = None,       # 生成数量
    ) -> Union[List[str], str]:
        """异步生成回复"""
        
        # 1. 设置默认参数
        if max_tokens is None:
            max_tokens = self.DEFAULT_MAX_TOKENS      # 1000
        if temperature is None:
            temperature = self.DEFAULT_TEMPERATURE     # 0.2
        if num_comps is None:
            num_comps = self.DEFUALT_NUM_COMPLETIONS  # 1
        
        # 2. 处理输入格式
        if isinstance(messages, str):
            # 如果输入是字符串，转换为消息格式
            messages = [{'role':'user', 'content': messages}]
        
        # 3. 调用achat函数
        return await achat(self.model_name, messages)
    
    def gen(self, messages: List[Message], **kwargs) -> Union[List[str], str]:
        """同步生成（未实现）"""
        pass
```

---

## 模型配置与初始化

### 1. 环境变量配置 (template.env)

```dotenv
# API基础URL
# - 本地vLLM服务: http://localhost:8000/v1
# - OpenAI官方: https://api.openai.com/v1
# - 其他兼容服务: 根据服务提供商设置
BASE_URL="http://localhost:8000/v1"

# API密钥
# - 本地服务通常使用: EMPTY
# - OpenAI官方: sk-xxxxxxxxxxxxx
API_KEY="EMPTY"
```

### 2. 使用本地vLLM服务

#### 启动本地模型服务

```bash
# 安装vLLM
pip install vllm

# 启动服务（以Qwen3-8B为例）
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --port 8000 \
  --api-key EMPTY
```

#### 配置AgentPrune

```dotenv
# .env文件
BASE_URL="http://localhost:8000/v1"
API_KEY="EMPTY"
```

```bash
# 运行实验
python experiments/run_gsm8k.py --llm_name Qwen2.5-7B-Instruct
```

### 3. 使用OpenAI官方API

```dotenv
# .env文件
BASE_URL="https://api.openai.com/v1"
API_KEY="sk-your-actual-openai-api-key"
```

```bash
# 运行实验
python experiments/run_gsm8k.py --llm_name gpt-4o
```

### 4. 模型初始化流程

```
run_gsm8k.py
  │
  ├──► 创建Graph时指定llm_name
  │      graph = Graph(llm_name="Qwen3-8B", ...)
  │
  ├──► Graph初始化节点
  │      self.init_nodes()
  │
  ├──► 创建MathSolver节点
  │      agent = AgentRegistry.get("MathSolver", llm_name="Qwen3-8B")
  │
  ├──► MathSolver.__init__()
  │      self.llm = LLMRegistry.get(llm_name)
  │                    │
  │                    ├──► LLMRegistry.get("Qwen3-8B")
  │                    │      └──► registry.get('GPTChat', "Qwen3-8B")
  │                    │
  │                    └──► GPTChat("Qwen3-8B")
  │                           └──► self.model_name = "Qwen3-8B"
  │
  └──► 节点拥有可用的LLM实例
         self.llm.agen(messages)
```

---

## Prompt构建机制

### 1. PromptSet架构

```python
from AgentPrune.prompt.prompt_set_registry import PromptSetRegistry

@PromptSetRegistry.register('gsm8k')
class GSM8KPromptSet(PromptSet):
    """GSM8K数学问题的Prompt集合"""
    
    # 1. 角色定义
    @staticmethod
    def get_role():
        """循环返回不同的角色"""
        return next(roles)  # 'Math Solver', 'Mathematical Analyst', ...
    
    # 2. 角色约束
    @staticmethod
    def get_constraint(role):
        """获取角色的系统提示"""
        return ROLE_DESCRIPTION[role]
    
    # 3. 答案生成提示
    @staticmethod
    def get_answer_prompt(question, role):
        """构建包含Few-Shot示例的提示"""
        return f"{FEW_SHOT_DATA[role]}\n\nQ:{question}"
    
    # 4. 决策节点提示
    @staticmethod
    def get_decision_constraint():
        """最终决策节点的约束"""
        return "Please find the most reliable answer..."
    
    @staticmethod
    def get_decision_role():
        """最终决策节点的角色"""
        return "You are the top decision-maker."
```

### 2. 角色系统

#### 角色定义

```python
roles = itertools.cycle([
    'Math Solver',           # 数学求解器
    'Mathematical Analyst',  # 数学分析师
    'Programming Expert',    # 编程专家
    'Inspector',            # 检查员
])
```

#### 角色描述 (ROLE_DESCRIPTION)

##### Math Solver

```python
"Math Solver": 
    "You are a math expert. "
    "You will be given a math problem and hints from other agents. "
    "Give your own solving process step by step based on hints. "
    "The last line of your output contains only the final result without any units, "
    "for example: The answer is 140\n"
    "You will be given some examples you may refer to."
```

**特点**：
- 接收其他智能体的提示
- 逐步展示求解过程
- 标准化输出格式："The answer is X"

##### Mathematical Analyst

```python
"Mathematical Analyst":
    "You are a mathematical analyst. "
    "You will be given a math problem, analysis and code from other agents. "
    "You need to first analyze the problem-solving process step by step, "
    "where the variables are represented by letters. "
    "Then you substitute the values into the analysis process to perform calculations "
    "and get the results."
    "The last line of your output contains only the final result without any units, "
    "for example: The answer is 140\n"
    "You will be given some examples you may refer to."
```

**特点**：
- 符号化分析问题
- 将具体数值代入抽象过程
- 强调过程的通用性

##### Programming Expert

```python
"Programming Expert":
    "You are a programming expert. "
    "You will be given a math problem, analysis and code from other agents. "
    "Integrate step-by-step reasoning and Python code to solve math problems. "
    "Analyze the question and write functions to solve the problem. "
    "The function should not take any arguments and use the final result as the return value. "
    "The last line of code calls the function you wrote and assigns the return value "
    "to the (answer) variable. "
    "Use a Python code block to write your response. For example:\n"
    "```python\n"
    "def fun():\n"
    "    x = 10\n"
    "    y = 20\n"
    "    return x + y\n"
    "answer = fun()\n"
    "```\n"
    "Do not include anything other than Python code blocks in your response."
    "You will be given some examples you may refer to."
```

**特点**：
- 生成可执行的Python代码
- 函数式编程风格
- 代码会实际执行并验证

##### Inspector

```python
"Inspector":
    "You are an Inspector. "
    "You will be given a math problem, analysis and code from other agents. "
    "Check whether the logic/calculation of the problem solving and analysis process "
    "is correct(if present). "
    "Check whether the code corresponds to the solution analysis(if present). "
    "Give your own solving process step by step based on hints. "
    "The last line of your output contains only the final result without any units, "
    "for example: The answer is 140\n"
    "You will be given some examples you may refer to."
```

**特点**：
- 验证其他智能体的推理
- 检查代码与分析的一致性
- 提供独立的解决方案

### 3. Few-Shot示例

#### Math Solver示例

```python
FEW_SHOT_DATA["Math Solver"] = """
Q: Angelo and Melanie want to plan how many hours over the next week they should 
study together for their test next week. They have 2 chapters of their textbook to 
study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours 
to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to 
study no more than 4 hours each day, how many days should they plan to study total 
over the next week if they take a 10-minute break every hour, include 3 10-minute 
snack breaks each day, and 30 minutes for lunch each day? 
(Hint: The answer is near to 4).

A: We know the Answer Hints: 4. With the Answer Hints: 4, we will answer the question. 
Let's think step by step. 
Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 
3 hours x 2 chapters = 6 hours total.
For the worksheets they plan to dedicate 1.5 hours for each worksheet, 
1.5 hours x 4 worksheets = 6 hours total.
Angelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 
12 / 4 = 3 days.
However, they need to include time for breaks and lunch. Every hour they want to 
include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.
They also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.
And they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 
30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 
180 / 60 minutes per hour = 3 extra hours.
So Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.
They want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75
They will need to plan to study 4 days to allow for all the time they need.
The answer is 4

[更多示例...]
"""
```

#### Programming Expert示例

```python
FEW_SHOT_DATA["Programming Expert"] = """
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?

A:
```python
def money_left():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels * bagel_cost
    remaining_money = money_initial - money_spent
    return remaining_money
 
answer = money_left()
```

Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, 
he lost 2 more. How many golf balls did he have at the end of wednesday?

A:
```python
def remaining_golf_balls():
    golf_balls_initial = 58
    golf_balls_lost_tuesday = 23
    golf_balls_lost_wednesday = 2
    golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday
    remaining_golf_balls = golf_balls_left
    return remaining_golf_balls

answer = remaining_golf_balls() 
```
"""
```

### 4. Prompt组装流程

#### 在MathSolver中

```python
def _process_inputs(self, raw_inputs, spatial_info, temporal_info, **kwargs):
    """组装最终的prompt"""
    
    # 1. 获取系统提示（角色约束）
    system_prompt = self.constraint
    # 例如："You are a math expert. You will be given..."
    
    # 2. 获取基础用户提示（包含Few-Shot）
    user_prompt = self.prompt_set.get_answer_prompt(
        question=raw_inputs["task"],
        role=self.role
    )
    # 例如：
    # """
    # [Few-Shot示例1]
    # [Few-Shot示例2]
    # 
    # Q: John has 5 apples...
    # """
    
    # 3. 处理不同角色的提示增强
    if self.role == "Math Solver":
        # Math Solver: 添加数值提示
        user_prompt += "(Hint: The answer is near to"
        for id, info in spatial_info.items():
            user_prompt += " " + gsm_get_predict(info["output"])
        for id, info in temporal_info.items():
            user_prompt += " " + gsm_get_predict(info["output"])
        user_prompt += ")."
        # 结果例如："(Hint: The answer is near to 3 3 3)."
    
    else:
        # 其他角色: 添加完整的上下文信息
        spatial_str = ""
        temporal_str = ""
        
        # 收集空间信息（同一轮其他智能体的输出）
        for id, info in spatial_info.items():
            spatial_str += (
                f"Agent {id} as a {info['role']} "
                f"his answer to this question is:\n\n"
                f"{info['output']}\n\n"
            )
        
        # 收集时间信息（上一轮智能体的输出）
        for id, info in temporal_info.items():
            temporal_str += (
                f"Agent {id} as a {info['role']} "
                f"his answer to this question was:\n\n"
                f"{info['output']}\n\n"
            )
        
        # 添加到提示中
        if len(spatial_str):
            user_prompt += (
                f"At the same time, there are the following responses "
                f"to the same question for your reference:\n\n{spatial_str}\n\n"
            )
        
        if len(temporal_str):
            user_prompt += (
                f"In the last round of dialogue, there were the following responses "
                f"to the same question for your reference:\n\n{temporal_str}"
            )
    
    return system_prompt, user_prompt
```

#### 完整Prompt示例

**输入场景**：
- 角色：Mathematical Analyst
- 问题：John has 5 apples. He gives 2 to Mary. How many does he have left?
- 空间信息：Agent A (Math Solver) 说 "5-2=3, The answer is 3"
- 时间信息：Agent B (上一轮) 说 "The answer is 3"

**生成的System Prompt**：
```
You are a mathematical analyst. You will be given a math problem, analysis and code 
from other agents. You need to first analyze the problem-solving process step by step, 
where the variables are represented by letters. Then you substitute the values into 
the analysis process to perform calculations and get the results. The last line of 
your output contains only the final result without any units, for example: The answer is 140
You will be given some examples you may refer to.
```

**生成的User Prompt**：
```
[Few-Shot示例1：关于树的问题]
[Few-Shot示例2：关于巧克力的问题]

Q: John has 5 apples. He gives 2 to Mary. How many does he have left?

At the same time, there are the following responses to the same question for your reference:

Agent xyz1 as a Math Solver his answer to this question is:

John started with 5 apples. He gave away 2 apples. 5 - 2 = 3. The answer is 3.

In the last round of dialogue, there were the following responses to the same question 
for your reference:

Agent xyz2 as a Math Solver his answer to this question was:

The answer is 3.
```

---

## 实际调用流程

### 完整调用链路

```
1. Node执行
   MathSolver._async_execute(input, spatial_info, temporal_info)
   
2. 处理输入
   system_prompt, user_prompt = self._process_inputs(...)
   
3. 构建消息
   message = [
       {'role': 'system', 'content': system_prompt},
       {'role': 'user', 'content': user_prompt}
   ]
   
4. 调用LLM
   response = await self.llm.agen(message)
                        │
                        ├──► GPTChat.agen(message)
                        │      │
                        │      ├──► 设置默认参数
                        │      │    max_tokens = 1000
                        │      │    temperature = 0.2
                        │      │
                        │      └──► await achat(self.model_name, message)
                        │              │
                        │              ├──► 创建AsyncOpenAI客户端
                        │              │    base_url = "http://localhost:8000/v1"
                        │              │    api_key = "EMPTY"
                        │              │
                        │              ├──► 发起HTTP POST请求
                        │              │    POST /v1/chat/completions
                        │              │    {
                        │              │      "model": "Qwen3-8B",
                        │              │      "messages": [
                        │              │        {"role": "system", "content": "..."},
                        │              │        {"role": "user", "content": "..."}
                        │              │      ]
                        │              │    }
                        │              │
                        │              ├──► vLLM服务处理
                        │              │    - 加载模型
                        │              │    - 编码输入
                        │              │    - 生成文本
                        │              │    - 返回响应
                        │              │
                        │              └──► 提取响应内容
                        │                   response = chat_completion.choices[0].message.content
                        │
                        └──► 返回: "John had 5 apples..."

5. 后处理（如果是Programming Expert）
   if self.role == "Programming Expert":
       answer = execute_code_get_return(response.lstrip("```python\n").rstrip("\n```"))
       response += f"\nthe answer is {answer}"

6. 保存结果
   node.outputs.append(response)
```

### 时序图

```
MathSolver                GPTChat              achat                vLLM服务
    │                        │                    │                      │
    │─────agen(msg)─────────>│                    │                      │
    │                        │                    │                      │
    │                        │──achat(model,msg)──>│                      │
    │                        │                    │                      │
    │                        │                    │──POST /v1/chat/...──>│
    │                        │                    │                      │
    │                        │                    │                      │ (模型推理)
    │                        │                    │                      │
    │                        │                    │<────response─────────│
    │                        │                    │                      │
    │                        │<───return text─────│                      │
    │                        │                    │                      │
    │<────return text────────│                    │                      │
    │                        │                    │                      │
```

---

## 完整调用链路追踪

### 场景：4个MathSolver解决一个数学问题

#### 初始状态

```python
# Graph配置
graph = Graph(
    domain="gsm8k",
    llm_name="Qwen3-8B",
    agent_names=['MathSolver', 'MathSolver', 'MathSolver', 'MathSolver'],
    decision_method='FinalRefer',
)

# 输入
input_dict = {"task": "John has 5 apples. He gives 2 to Mary. How many does he have left?"}
```

#### Round 0 开始

##### Node A (第一个执行)

```
1. get_spatial_info() -> {}  (没有前驱节点)
2. get_temporal_info() -> {}  (第一轮)

3. _process_inputs()
   system_prompt = "You are a math expert..."
   user_prompt = """
   [Few-Shot示例]
   
   Q: John has 5 apples. He gives 2 to Mary. How many does he have left?
   """

4. llm.agen([
     {'role': 'system', 'content': 'You are a math expert...'},
     {'role': 'user', 'content': '[Few-Shot]\nQ: John has 5 apples...'}
   ])

5. achat("Qwen3-8B", messages)
   
   HTTP Request:
   POST http://localhost:8000/v1/chat/completions
   {
     "model": "Qwen3-8B",
     "messages": [
       {"role": "system", "content": "You are a math expert..."},
       {"role": "user", "content": "[Few-Shot]\nQ: John has 5 apples..."}
     ]
   }
   
   vLLM Response:
   {
     "choices": [{
       "message": {
         "content": "Let's think step by step.\nJohn has 5 apples initially.\n
                     He gives 2 apples to Mary.\n5 - 2 = 3.\n
                     John has 3 apples left.\nThe answer is 3"
       }
     }]
   }

6. node.outputs = ["Let's think step by step...\nThe answer is 3"]
```

##### Node B (依赖Node A)

```
1. get_spatial_info() -> {
     "node_a_id": {
       "role": "Math Solver",
       "output": "Let's think step by step...\nThe answer is 3"
     }
   }
2. get_temporal_info() -> {}

3. _process_inputs()
   system_prompt = "You are a math expert..."
   user_prompt = """
   [Few-Shot示例]
   
   Q: John has 5 apples. He gives 2 to Mary. How many does he have left?
   (Hint: The answer is near to 3).
   """
   # 注意：Math Solver角色会提取前驱节点的答案作为提示

4. llm.agen(messages)

5. vLLM Response:
   "Based on the hint that the answer is near 3, let's verify.\n
    John has 5 apples. Giving 2 to Mary: 5 - 2 = 3.\n
    The answer is 3"

6. node.outputs = ["Based on the hint...\nThe answer is 3"]
```

##### Node C, Node D (类似流程)

```
Node C:
  spatial_info: {node_a, node_b的输出}
  输出: "Considering other agents' answers, 5-2=3. The answer is 3"

Node D:
  spatial_info: {node_a, node_b, node_c的输出}
  输出: "All agents agree. The answer is 3"
```

#### update_memory()

```python
for node in [A, B, C, D]:
    node.last_memory = {
        'inputs': node.inputs,
        'outputs': node.outputs,
        'raw_inputs': node.raw_inputs
    }

# 现在每个节点都保存了这一轮的输出
```

#### Round 1 开始（如果num_rounds=2）

##### Node A (可能有时间连接)

```
假设temporal连接: Node D(t-1) -> Node A(t)

1. get_spatial_info() -> {...}  (当前轮的空间信息)
2. get_temporal_info() -> {
     "node_d_id": {
       "role": "Math Solver",
       "output": "All agents agree. The answer is 3"  # 来自last_memory
     }
   }

3. _process_inputs()
   user_prompt包含：
   - Few-Shot示例
   - 原问题
   - 空间信息（当前轮其他节点）
   - 时间信息（上一轮Node D说"All agents agree..."）

4. llm.agen(messages)

5. 输出: "Considering feedback from previous round, confirmed: The answer is 3"
```

#### 决策节点 (FinalRefer)

```
1. connect_decision_node()
   # 所有节点连接到decision_node

2. decision_node.async_execute(input)

3. get_spatial_info() -> {
     "node_a": {"output": "...The answer is 3"},
     "node_b": {"output": "...The answer is 3"},
     "node_c": {"output": "...The answer is 3"},
     "node_d": {"output": "...The answer is 3"}
   }

4. _process_inputs()
   system_prompt = "You are the top decision-maker..."
   user_prompt = """
   [Few-Shot决策示例]
   
   The task is:
   John has 5 apples. He gives 2 to Mary. How many does he have left?
   
   At the same time, the output of other agents is as follows:
   
   node_a: Let's think step by step...The answer is 3
   
   node_b: Based on the hint...The answer is 3
   
   node_c: Considering other agents' answers...The answer is 3
   
   node_d: All agents agree. The answer is 3
   """

5. llm.agen(messages)

6. vLLM Response:
   "All four agents have independently arrived at the same answer through 
    step-by-step reasoning. The calculation 5 - 2 = 3 is straightforward 
    and correct. The answer is 3"

7. decision_node.outputs = ["All four agents...The answer is 3"]
```

#### 返回结果

```python
final_answers = decision_node.outputs
log_probs = 累积的所有连接概率的对数

return (
    ["All four agents...The answer is 3"],  # final_answers
    -2.34                                    # log_probs（示例值）
)
```

---

## 关键技术细节

### 1. 异步并发

```python
# 批处理中的并发
answer_log_probs = []
for record in current_batch:
    realized_graph = copy.deepcopy(graph)
    task = asyncio.create_task(realized_graph.arun(input_dict, num_rounds))
    answer_log_probs.append(task)

# 等待所有任务完成
raw_results = await asyncio.gather(*answer_log_probs)
```

**优点**：
- 同时处理批次中的多个问题
- 充分利用异步I/O
- 显著加速实验

### 2. 重试机制

```python
@retry(wait=wait_random_exponential(max=300), stop=stop_after_attempt(3))
async def achat(model: str, msg: List[Dict]):
    ...
```

**处理的错误**：
- 网络超时
- API限流（429错误）
- 临时服务不可用（503错误）
- 其他临时性错误

### 3. 参数配置

```python
# 默认参数
DEFAULT_MAX_TOKENS = 1000      # 控制输出长度
DEFAULT_TEMPERATURE = 0.2      # 0.2较低，输出更确定性
DEFUALT_NUM_COMPLETIONS = 1    # 只生成1个回复
```

**Temperature说明**：
- `0.0`：完全确定性，总是选择最可能的token
- `0.2`：**当前设置**，较为确定，适合数学推理
- `0.7`：平衡创造性和确定性
- `1.0`：标准采样
- `>1.5`：非常随机，适合创意写作

### 4. 成本追踪（未完全实现）

```python
# price.py中的函数
def cost_count(prompt, response, model_name):
    prompt_len = cal_token(model_name, prompt)
    completion_len = cal_token(model_name, response)
    
    # 根据模型计算价格
    if "gpt-4" in model_name:
        price = prompt_len * 0.01/1000 + completion_len * 0.03/1000
    
    # 累积到全局
    Cost.instance().value += price
    PromptTokens.instance().value += prompt_len
    CompletionTokens.instance().value += completion_len
```

**注意**：当前代码中 `cost_count` 函数已导入但未在实际调用中使用。

---

## 扩展与定制

### 1. 添加新的LLM实现

```python
from AgentPrune.llm.llm import LLM
from AgentPrune.llm.llm_registry import LLMRegistry

@LLMRegistry.register('Claude')
class ClaudeLLM(LLM):
    def __init__(self, model_name: str):
        self.model_name = model_name
        # 初始化Anthropic客户端
        
    async def agen(self, messages, **kwargs):
        # 实现Claude API调用
        pass
```

### 2. 添加新的角色

```python
# 在gsm8k_prompt_set.py中
roles = itertools.cycle([
    'Math Solver',
    'Mathematical Analyst',
    'Programming Expert',
    'Inspector',
    'Your New Role',  # 添加新角色
])

ROLE_DESCRIPTION["Your New Role"] = """
Your role description here...
"""

FEW_SHOT_DATA["Your New Role"] = """
Your few-shot examples here...
"""
```

### 3. 自定义Prompt模板

```python
@PromptSetRegistry.register('your_dataset')
class YourPromptSet(PromptSet):
    @staticmethod
    def get_answer_prompt(question, role):
        # 自定义prompt构建逻辑
        return f"Your custom template for {question}"
```

---

## 调试技巧

### 1. 查看实际发送的Prompt

```python
# 在MathSolver._async_execute()中添加
system_prompt, user_prompt = self._process_inputs(input, spatial_info, temporal_info)
print(f"=== System Prompt ===\n{system_prompt}\n")
print(f"=== User Prompt ===\n{user_prompt}\n")
message = [{'role':'system','content':system_prompt},{'role':'user','content':user_prompt}]
response = await self.llm.agen(message)
print(f"=== Response ===\n{response}\n")
```

### 2. 测试单个LLM调用

```python
import asyncio
from AgentPrune.llm.llm_registry import LLMRegistry

async def test():
    llm = LLMRegistry.get("Qwen3-8B")
    response = await llm.agen([
        {'role': 'user', 'content': 'What is 2+2?'}
    ])
    print(response)

asyncio.run(test())
```

### 3. 检查API连接

```bash
# 测试vLLM服务是否正常
curl http://localhost:8000/v1/models

# 测试chat completions
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen3-8B",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

---

## 常见问题

### Q1: 如何更换模型？

**A**: 修改命令行参数
```bash
python experiments/run_gsm8k.py --llm_name "your-model-name"
```

模型名称必须与vLLM服务中加载的模型名称一致。

### Q2: 如何调整生成参数？

**A**: 修改LLM类的默认值
```python
# 在llm.py中
class LLM(ABC):
    DEFAULT_MAX_TOKENS = 2000        # 增加输出长度
    DEFAULT_TEMPERATURE = 0.5        # 增加随机性
```

或在调用时传递参数（需要修改代码）：
```python
response = await self.llm.agen(message, temperature=0.5, max_tokens=2000)
```

### Q3: 如何使用OpenAI官方API？

**A**: 
1. 修改 `.env` 文件：
```dotenv
BASE_URL="https://api.openai.com/v1"
API_KEY="sk-your-openai-api-key"
```

2. 运行实验：
```bash
python experiments/run_gsm8k.py --llm_name gpt-4o
```

### Q4: 如何添加token计数？

**A**: 在 `achat` 函数中添加：
```python
async def achat(model: str, msg: List[Dict]):
    client = AsyncOpenAI(base_url=MINE_BASE_URL, api_key=MINE_API_KEY)
    chat_completion = await client.chat.completions.create(messages=msg, model=model)
    response = chat_completion.choices[0].message.content
    
    # 计算token和成本
    prompt = str(msg)
    cost_count(prompt, response, model)
    
    return response
```

### Q5: 如何处理超时？

**A**: `achat` 函数已经有重试机制。如果仍然超时，可以调整：
```python
@retry(wait=wait_random_exponential(max=600), stop=stop_after_attempt(5))
async def achat(model: str, msg: List[Dict]):
    ...
```

---

## 总结

AgentPrune的模型调用系统设计精巧，具有以下特点：

1. **分层架构**：抽象层、注册层、实现层分离，易于扩展
2. **统一接口**：支持多种LLM后端（OpenAI、vLLM、其他兼容API）
3. **异步高效**：使用asyncio实现并发调用
4. **容错机制**：自动重试处理临时错误
5. **Prompt工程**：丰富的角色系统和Few-Shot示例
6. **灵活配置**：通过环境变量和命令行参数轻松切换模型

这个设计使得系统既能在本地部署的开源模型上运行，也能无缝切换到商业API，为研究和生产提供了极大的灵活性。
