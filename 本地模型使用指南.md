# 使用本地Hugging Face模型指南

## 概述

现在AgentPrune支持使用Hugging Face Transformers直接加载本地模型，无需启动API服务。

## 安装依赖

```bash
# 激活环境
conda activate ai

# 安装transformers和相关依赖
pip install transformers accelerate torch
```

## 模型准备

确保模型已下载到本地。假设模型在以下位置：
```
AgentPrune/
├── Qwen/
│   └── Qwen3-8B/
│       ├── config.json
│       ├── tokenizer_config.json
│       ├── pytorch_model.bin (或 .safetensors)
│       └── ...
```

## 使用方法

### 方法1: 直接在命令行指定路径

```bash
# 使用相对路径（相对于AgentPrune根目录）
python experiments/run_gsm8k.py --llm_name "Qwen/Qwen3-8B"

# 或使用 ./ 开头的相对路径
python experiments/run_gsm8k.py --llm_name "./Qwen/Qwen3-8B"

# 或使用绝对路径
python experiments/run_gsm8k.py --llm_name "D:/GitHub/AgentPrune/Qwen/Qwen3-8B"
```

### 方法2: 在代码中使用

```python
from AgentPrune.llm.llm_registry import LLMRegistry

# 使用相对路径
llm = LLMRegistry.get("Qwen/Qwen3-8B")

# 或使用绝对路径
llm = LLMRegistry.get("D:/path/to/model")

# 异步调用
messages = [
    {'role': 'system', 'content': 'You are a helpful assistant.'},
    {'role': 'user', 'content': 'What is 2+2?'}
]
response = await llm.agen(messages)
```

## 工作原理

### 自动识别机制

`LLMRegistry.get()` 会自动判断传入的是本地路径还是API模型名：

```python
# 本地路径 → 使用 LocalTransformers
LLMRegistry.get("Qwen/Qwen3-8B")        # 包含 /
LLMRegistry.get("./models/qwen")         # 以 ./ 开头
LLMRegistry.get("D:/models/qwen")        # 绝对路径

# API模型名 → 使用 GPTChat
LLMRegistry.get("gpt-4o")                # 无路径分隔符
LLMRegistry.get("gpt-3.5-turbo")         # 标准模型名
```

### 性能优化

1. **模型缓存**: 同一模型只加载一次，后续调用直接使用缓存
2. **延迟加载**: 只在第一次调用时加载模型
3. **自动GPU加速**: 如果有CUDA可用，自动使用GPU
4. **异步支持**: 使用线程池避免阻塞事件循环

## 完整示例

### 测试本地模型

```bash
# 运行测试脚本
python test_local_model.py
```

### 运行GSM8K实验

```bash
# 使用本地Qwen3-8B模型
python experiments/run_gsm8k.py \
  --llm_name "Qwen/Qwen3-8B" \
  --batch_size 4 \
  --num_rounds 1 \
  --mode FullConnected

# 使用本地模型 + 优化
python experiments/run_gsm8k.py \
  --llm_name "Qwen/Qwen3-8B" \
  --optimized_spatial \
  --pruning_rate 0.25 \
  --num_iterations 20
```

## 参数说明

### LocalTransformers 参数

```python
await llm.agen(
    messages,
    max_tokens=1000,      # 最大生成token数，默认1000
    temperature=0.2,      # 温度参数，默认0.2（较低=更确定）
    num_comps=1,         # 生成数量（暂不支持多个）
)
```

### 生成参数

在 `local_transformers.py` 的 `_generate` 方法中配置：

```python
outputs = self.model.generate(
    **inputs,
    max_new_tokens=max_tokens,     # 最大新生成token数
    temperature=temperature,        # 温度
    do_sample=temperature > 0,      # 是否采样
    top_p=0.95,                    # nucleus采样
    repetition_penalty=1.1,        # 重复惩罚
    pad_token_id=self.tokenizer.eos_token_id,
)
```

可以根据需要调整这些参数。

## 与API模式对比

| 特性 | LocalTransformers | GPTChat (API) |
|------|-------------------|---------------|
| 网络需求 | 不需要 | 需要 |
| 启动速度 | 首次较慢（加载模型） | 快速 |
| 推理速度 | 取决于硬件 | 取决于网络 |
| 成本 | 免费（需要GPU） | 按token计费 |
| 隐私 | 完全本地 | 数据上传 |
| 模型选择 | 任何HF模型 | API提供的模型 |

## 内存和GPU要求

### Qwen3-8B 模型

- **GPU内存**: 
  - FP16: ~16GB
  - INT8: ~8GB
  - INT4: ~4GB
- **CPU内存**: ~32GB（无GPU时）

### 优化建议

如果GPU内存不足，可以修改 `local_transformers.py`：

```python
# 使用8bit量化
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

self.model = AutoModelForCausalLM.from_pretrained(
    self.model_path,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

## 支持的模型

理论上支持所有Hugging Face上的对话模型：

- Qwen系列: Qwen, Qwen2, Qwen2.5
- LLaMA系列: LLaMA-2, LLaMA-3
- Mistral系列
- 其他支持 `AutoModelForCausalLM` 的模型

## 故障排除

### 问题1: 模型加载失败

```
ValueError: 模型路径不存在: ...
```

**解决方案**:
- 检查路径是否正确
- 确保模型文件完整下载
- 使用绝对路径测试

### 问题2: CUDA Out of Memory

```
RuntimeError: CUDA out of memory
```

**解决方案**:
- 减小batch_size
- 使用量化（8bit或4bit）
- 使用CPU模式（较慢）

### 问题3: 生成内容格式不对

**解决方案**:
检查 `_format_messages` 方法，确保与模型的对话格式匹配。不同模型可能需要不同的格式。

## 切换回API模式

如果需要切换回API模式：

```bash
# 使用OpenAI API
python experiments/run_gsm8k.py --llm_name "gpt-4o"

# 使用本地vLLM服务
# 1. 确保 .env 配置正确
# 2. 使用标准模型名（无路径分隔符）
python experiments/run_gsm8k.py --llm_name "Qwen3-8B"
```

## 调试技巧

### 1. 打印生成的prompt

在 `local_transformers.py` 的 `_generate` 方法中添加：

```python
prompt = self._format_messages(messages)
print("=" * 80)
print("Formatted Prompt:")
print(prompt)
print("=" * 80)
```

### 2. 检查模型输出

```python
print(f"Generated IDs: {generated_ids}")
print(f"Response: {response}")
```

### 3. 监控GPU使用

```bash
# 另一个终端运行
watch -n 1 nvidia-smi
```

## 性能调优

### 1. 批量推理

目前实现是逐个处理，如需批量处理可以修改代码支持batch inference。

### 2. KV Cache优化

对于多轮对话，可以保存KV cache加速推理。

### 3. 量化加速

使用GPTQ或AWQ量化模型可以显著提升速度。

## 总结

现在你可以：

✅ 直接使用本地Hugging Face模型  
✅ 无需启动额外的API服务  
✅ 保护数据隐私  
✅ 节省API成本  
✅ 支持离线运行  

只需在命令行中指定模型路径即可！
